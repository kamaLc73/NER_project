{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Download/Extract Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset déjà existant.\n"
     ]
    }
   ],
   "source": [
    "def download_extract(url, extract_path):\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "    filename = os.path.join(extract_path, 'dataset.tar.gz')\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Téléchargement du dataset...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(\"Extraction du dataset...\")\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_path)\n",
    "        print(\"Dataset prêt !\")\n",
    "    else:\n",
    "        print(\"Dataset déjà existant.\")\n",
    "\n",
    "conll_tar_url = 'http://lnsigo.mipt.ru/export/datasets/conll2003.tar.gz'\n",
    "download_path = 'conll2003'\n",
    "download_extract(conll_tar_url, download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases dans train: 14041\n",
      "Nombre de phrases dans test: 3453\n",
      "Nombre de phrases dans valid: 3250\n"
     ]
    }
   ],
   "source": [
    "data_types = ['train', 'test', 'valid']\n",
    "dataset_dict = {}\n",
    "\n",
    "for data_type in data_types:\n",
    "    filepath = os.path.join(download_path, f'{data_type}.txt')\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        dataset_dict[data_type] = []\n",
    "        sentences, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('-DOCSTART-'):\n",
    "                parts = line.split()\n",
    "                if len(parts) == 2:\n",
    "                    token, tag = parts\n",
    "                    sentences.append(token)\n",
    "                    tags.append(tag)\n",
    "            elif sentences:\n",
    "                dataset_dict[data_type].append((sentences, tags))\n",
    "                sentences, tags = [], []\n",
    "\n",
    "for key in dataset_dict:\n",
    "    print(f'Nombre de phrases dans {key}: {len(dataset_dict[key])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Creation and Encoding/Padding Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "tag_vocab = {\"<PAD>\": 0}\n",
    "\n",
    "for data_type in ['train']:\n",
    "    for tokens, tags in dataset_dict[data_type]:\n",
    "        for token in tokens:\n",
    "            if token not in word_vocab:\n",
    "                word_vocab[token] = len(word_vocab)\n",
    "        for tag in tags:\n",
    "            if tag not in tag_vocab:\n",
    "                tag_vocab[tag] = len(tag_vocab)\n",
    "\n",
    "def encode_and_pad(data, vocab, tag_vocab, max_len):\n",
    "    X = [[vocab.get(token, vocab[\"<UNK>\"]) for token in tokens] for tokens, _ in data]\n",
    "    y = [[tag_vocab[tag] for tag in tags] for _, tags in data]\n",
    "    X = pad_sequences(X, maxlen=max_len, padding=\"post\")\n",
    "    y = pad_sequences(y, maxlen=max_len, padding=\"post\")\n",
    "    return X, y\n",
    "\n",
    "max_len = max(len(tokens) for data_type in dataset_dict for tokens, _ in dataset_dict[data_type])\n",
    "\n",
    "X_train, y_train = encode_and_pad(dataset_dict[\"train\"], word_vocab, tag_vocab, max_len)\n",
    "X_test, y_test = encode_and_pad(dataset_dict[\"test\"], word_vocab, tag_vocab, max_len)\n",
    "X_val, y_val = encode_and_pad(dataset_dict[\"valid\"], word_vocab, tag_vocab, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m2,362,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m234,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,599,566</span> (9.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,599,566\u001b[0m (9.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,599,566</span> (9.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,599,566\u001b[0m (9.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_length = max_len\n",
    "n_words = len(word_vocab)\n",
    "n_tags = len(tag_vocab)\n",
    "output_dim = 100\n",
    "lstm_units = 128\n",
    "\n",
    "input_layer = Input(shape=(input_length,))\n",
    "embedding_layer = Embedding(input_dim=n_words, output_dim=output_dim)(input_layer)\n",
    "lstm_layer = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(embedding_layer)\n",
    "dropout_layer = Dropout(0.5)(lstm_layer)\n",
    "output_layer = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(dropout_layer)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m7021/7021\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 39ms/step - accuracy: 0.9834 - loss: 0.0671 - val_accuracy: 0.9946 - val_loss: 0.0186\n",
      "Epoch 2/6\n",
      "\u001b[1m7021/7021\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 38ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 0.9958 - val_loss: 0.0151\n",
      "Epoch 3/6\n",
      "\u001b[1m7021/7021\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 38ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 0.9960 - val_loss: 0.0157\n",
      "Epoch 4/6\n",
      "\u001b[1m7021/7021\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 38ms/step - accuracy: 0.9996 - loss: 0.0014 - val_accuracy: 0.9958 - val_loss: 0.0169\n",
      "\u001b[1m1727/1727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9938 - loss: 0.0250\n",
      "Test Loss: 0.02411620318889618\n",
      "Test Accuracy: 0.9941114187240601\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "batch_size = 2\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, np.expand_dims(y_train, -1),\n",
    "    validation_data=(X_val, np.expand_dims(y_val, -1)),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[callback]\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, np.expand_dims(y_test, -1), batch_size=batch_size)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "model.save(\"saves/model_pretrained.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving tags and vocabs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saves/word_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_vocab, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"saves/tag_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tag_vocab, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 896ms/step\n",
      "[('James', 'B-PER'), ('Bond', 'I-PER'), ('works', 'O'), ('at', 'O'), ('Google', 'B-LOC'), ('INC', 'I-LOC'), ('in', 'O'), ('New', 'B-LOC'), ('York.', 'I-LOC')]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "[('Donald', 'B-PER'), ('Trump', 'I-PER'), ('was', 'O'), ('the', 'O'), ('president', 'O'), ('of', 'O'), ('the', 'O'), ('United', 'B-ORG'), ('States.', 'I-ORG')]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"saves/model_pretrained.keras\")\n",
    "\n",
    "with open(\"saves/word_vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_vocab = json.load(f)\n",
    "\n",
    "with open(\"saves/tag_vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tag_vocab = json.load(f)\n",
    "\n",
    "reverse_tag_vocab = {v: k for k, v in tag_vocab.items()}\n",
    "\n",
    "def predict_entities(sentence):\n",
    "    words = sentence.split()\n",
    "    encoded_sentence = [word_vocab.get(word, word_vocab[\"<UNK>\"]) for word in words]\n",
    "    padded_sentence = pad_sequences([encoded_sentence], maxlen=max_len, padding=\"post\")  \n",
    "    predictions = model.predict(padded_sentence)\n",
    "\n",
    "    predicted_tags = []\n",
    "    for prediction in predictions[0]:\n",
    "        tag_index = np.argmax(prediction)\n",
    "        predicted_tags.append(reverse_tag_vocab.get(tag_index))\n",
    "\n",
    "    aligned_tags = predicted_tags[:len(words)]\n",
    "\n",
    "    return list(zip(words, aligned_tags))\n",
    "\n",
    "sentence = \"James Bond works at Google INC in New York.\"\n",
    "entities = predict_entities(sentence)\n",
    "print(entities)\n",
    "\n",
    "sentence2 = \"Donald Trump was the president of the United States.\"\n",
    "entities2 = predict_entities(sentence2)\n",
    "print(entities2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
