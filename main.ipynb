{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists.\n",
      "Number of sentences in train: 14041\n",
      "Number of sentences in test: 3453\n",
      "Number of sentences in valid: 3250\n",
      "\n",
      "First two samples from the training set:\n",
      "Sentence 1:\n",
      "Tokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Tags:   ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "Sentence 2:\n",
      "Tokens: ['Peter', 'Blackburn']\n",
      "Tags:   ['B-PER', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Download and extract dataset\n",
    "def download_untar(url, extract_path):\n",
    "    if not os.path.exists(extract_path):\n",
    "        os.makedirs(extract_path, exist_ok=True)\n",
    "        filename = os.path.join(extract_path, 'dataset.tar.gz')\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_path)\n",
    "        print(\"Dataset ready!\")\n",
    "    else:\n",
    "        print(\"Dataset already exists.\")\n",
    "\n",
    "# Download the dataset\n",
    "conll_tar_url = 'http://lnsigo.mipt.ru/export/datasets/conll2003.tar.gz'\n",
    "download_path = 'conll2003/'\n",
    "download_untar(conll_tar_url, download_path)\n",
    "\n",
    "# Preprocess dataset\n",
    "data_types = ['train', 'test', 'valid']\n",
    "dataset_dict = dict()\n",
    "\n",
    "for data_type in data_types:\n",
    "    with open(f'{download_path}{data_type}.txt', 'r') as f:\n",
    "        xy_list = []\n",
    "        tokens, tags = [], []\n",
    "        for line in f:\n",
    "            items = line.split()\n",
    "            if len(items) > 1 and '-DOCSTART-' not in items[0]:\n",
    "                token, tag = items\n",
    "                tokens.append(token)\n",
    "                tags.append(tag)\n",
    "            elif tokens:\n",
    "                xy_list.append((tokens, tags))\n",
    "                tokens, tags = [], []\n",
    "        dataset_dict[data_type] = xy_list\n",
    "\n",
    "# Display dataset statistics\n",
    "for key in dataset_dict:\n",
    "    print(f'Number of sentences in {key}: {len(dataset_dict[key])}')\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nFirst two samples from the training set:\")\n",
    "for i, (tokens, tags) in enumerate(dataset_dict['train'][:2]):\n",
    "    print(f\"Sentence {i + 1}:\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Tags:   {tags}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data for model\n",
    "def encode_data(data, vocab, tag_vocab):\n",
    "    X, y = [], []\n",
    "    for tokens, tags in data:\n",
    "        X.append([vocab.get(token, vocab[\"<UNK>\"]) for token in tokens])\n",
    "        y.append([tag_vocab[tag] for tag in tags])\n",
    "    return X, y\n",
    "\n",
    "# Create vocabularies\n",
    "word_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "tag_vocab = {\"<PAD>\": 0}\n",
    "for tokens, tags in dataset_dict[\"train\"]:\n",
    "    for token in tokens:\n",
    "        if token not in word_vocab:\n",
    "            word_vocab[token] = len(word_vocab)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_vocab:\n",
    "            tag_vocab[tag] = len(tag_vocab)\n",
    "\n",
    "# Encode dataset\n",
    "X_train, y_train = encode_data(dataset_dict[\"train\"], word_vocab, tag_vocab)\n",
    "X_test, y_test = encode_data(dataset_dict[\"test\"], word_vocab, tag_vocab)\n",
    "\n",
    "# Pad sequences dynamically\n",
    "all_data = X_train + X_test\n",
    "max_len = max(len(seq) for seq in all_data)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len, padding=\"post\")\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train, maxlen=max_len, padding=\"post\")\n",
    "y_test = tf.keras.preprocessing.sequence.pad_sequences(y_test, maxlen=max_len, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m2,362,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m234,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,599,566</span> (9.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,599,566\u001b[0m (9.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,599,566</span> (9.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,599,566\u001b[0m (9.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model parameters\n",
    "input_length = max_len\n",
    "n_words = len(word_vocab)\n",
    "n_tags = len(tag_vocab)\n",
    "output_dim = 100  # Word embedding size\n",
    "lstm_units = 128  # LSTM units\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(input_length,))\n",
    "embedding_layer = Embedding(input_dim=n_words, output_dim=output_dim)(input_layer)\n",
    "lstm_layer = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(embedding_layer)\n",
    "dropout_layer = Dropout(0.5)(lstm_layer)\n",
    "output_layer = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(dropout_layer)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1756/1756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 53ms/step - accuracy: 0.9744 - loss: 0.1079 - val_accuracy: 0.9919 - val_loss: 0.0268\n",
      "Epoch 2/5\n",
      "\u001b[1m1756/1756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 55ms/step - accuracy: 0.9969 - loss: 0.0109 - val_accuracy: 0.9937 - val_loss: 0.0222\n",
      "Epoch 3/5\n",
      "\u001b[1m1756/1756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 54ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 0.9935 - val_loss: 0.0250\n",
      "Epoch 4/5\n",
      "\u001b[1m1756/1756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 53ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 0.9941 - val_loss: 0.0237\n",
      "Epoch 5/5\n",
      "\u001b[1m1756/1756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 55ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9934 - val_loss: 0.0280\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 8\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, np.expand_dims(y_train, -1),\n",
    "    validation_data=(X_test, np.expand_dims(y_test, -1)),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m432/432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9929 - loss: 0.0295\n",
      "Test Loss: 0.027967985719442368\n",
      "Test Accuracy: 0.9934041500091553\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, np.expand_dims(y_test, -1), batch_size=batch_size)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath=\"./model_pretrained.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
